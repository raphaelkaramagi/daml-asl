{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Comparison & Evaluation\n",
        "\n",
        "This notebook compares both approaches:\n",
        "- **Approach 1**: ResNet50 Transfer Learning on raw images\n",
        "- **Approach 2**: Neural Network on MediaPipe landmarks\n",
        "\n",
        "We'll evaluate both models on the test set and compare their performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import joblib\n",
        "import os\n",
        "import cv2\n",
        "import mediapipe as mp\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Models and Prepare Test Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "TEST_DATA_DIR = '../data/asl_alphabet_test/asl_alphabet_test'\n",
        "IMAGE_SIZE = (96, 96)\n",
        "\n",
        "# Model paths\n",
        "RESNET_MODEL_PATH = '../best_asl_resnet50_phase2.h5'  # or 'asl_resnet50_final.h5'\n",
        "NN_MODEL_PATH = '../data/nn_landmark_model.keras'\n",
        "LABEL_ENCODER_PATH = '../data/label_encoder.joblib'\n",
        "SCALER_PATH = '../data/scaler.joblib'\n",
        "\n",
        "# Class names (alphabetically ordered as in training)\n",
        "class_names = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', \n",
        "               'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', \n",
        "               'del', 'nothing', 'space']\n",
        "print(f\"Classes: {len(class_names)} classes\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load test images and labels\n",
        "print(\"Loading test images...\")\n",
        "test_images = []\n",
        "test_labels = []\n",
        "\n",
        "for filename in sorted(os.listdir(TEST_DATA_DIR)):\n",
        "    if filename.endswith('.jpg') or filename.endswith('.png'):\n",
        "        # Extract label from filename (e.g., 'A_test.jpg' -> 'A')\n",
        "        label = filename.replace('_test.jpg', '').replace('_test.png', '')\n",
        "        \n",
        "        # Read and preprocess image\n",
        "        img_path = os.path.join(TEST_DATA_DIR, filename)\n",
        "        img = cv2.imread(img_path)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        img = cv2.resize(img, IMAGE_SIZE)\n",
        "        \n",
        "        test_images.append(img)\n",
        "        test_labels.append(label)\n",
        "\n",
        "test_images = np.array(test_images)\n",
        "print(f\"Loaded {len(test_images)} test images\")\n",
        "print(f\"Test labels: {test_labels}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Approach 1: ResNet50 Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load ResNet50 model\n",
        "if os.path.exists(RESNET_MODEL_PATH):\n",
        "    print(f\"Loading ResNet50 model from {RESNET_MODEL_PATH}...\")\n",
        "    resnet_model = keras.models.load_model(RESNET_MODEL_PATH)\n",
        "    \n",
        "    # Prepare test images (normalize to 0-1)\n",
        "    test_images_normalized = test_images / 255.0\n",
        "    \n",
        "    # Make predictions\n",
        "    print(\"Making predictions...\")\n",
        "    predictions = resnet_model.predict(test_images_normalized)\n",
        "    predicted_indices = np.argmax(predictions, axis=1)\n",
        "    predicted_labels = [class_names[i] for i in predicted_indices]\n",
        "    \n",
        "    # Calculate accuracy\n",
        "    resnet_accuracy = accuracy_score(test_labels, predicted_labels)\n",
        "    print(f\"\\n✓ ResNet50 Test Accuracy: {resnet_accuracy*100:.2f}%\")\n",
        "    \n",
        "    # Show predictions\n",
        "    print(\"\\nPredictions:\")\n",
        "    for true_label, pred_label, conf in zip(test_labels, predicted_labels, np.max(predictions, axis=1)):\n",
        "        status = \"✓\" if true_label == pred_label else \"✗\"\n",
        "        print(f\"  {status} True: {true_label:8s} | Predicted: {pred_label:8s} | Confidence: {conf*100:.2f}%\")\n",
        "else:\n",
        "    print(f\"ResNet50 model not found at {RESNET_MODEL_PATH}\")\n",
        "    print(\"Please train the model first using notebook 05.\")\n",
        "    resnet_accuracy = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Approach 2: Landmark-based NN Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract landmarks from test images\n",
        "print(\"Extracting landmarks from test images...\")\n",
        "mp_hands = mp.solutions.hands\n",
        "hands_model = mp_hands.Hands(static_image_mode=True, max_num_hands=1, min_detection_confidence=0.5)\n",
        "\n",
        "test_landmarks = []\n",
        "for img in test_images:\n",
        "    results = hands_model.process(img)\n",
        "    \n",
        "    if results.multi_hand_landmarks:\n",
        "        hand_landmarks = results.multi_hand_landmarks[0]\n",
        "        wrist_coords = hand_landmarks.landmark[0]\n",
        "        \n",
        "        landmark_row = []\n",
        "        for landmark in hand_landmarks.landmark:\n",
        "            relative_x = landmark.x - wrist_coords.x\n",
        "            relative_y = landmark.y - wrist_coords.y\n",
        "            relative_z = landmark.z - wrist_coords.z\n",
        "            landmark_row.extend([relative_x, relative_y, relative_z])\n",
        "        test_landmarks.append(landmark_row)\n",
        "    else:\n",
        "        # If no landmarks detected, use zeros\n",
        "        test_landmarks.append([0] * 63)\n",
        "\n",
        "test_landmarks = np.array(test_landmarks)\n",
        "hands_model.close()\n",
        "print(f\"Extracted landmarks: {test_landmarks.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load landmark-based NN model\n",
        "if os.path.exists(NN_MODEL_PATH) and os.path.exists(LABEL_ENCODER_PATH) and os.path.exists(SCALER_PATH):\n",
        "    print(f\"Loading landmark model from {NN_MODEL_PATH}...\")\n",
        "    nn_model = keras.models.load_model(NN_MODEL_PATH)\n",
        "    label_encoder = joblib.load(LABEL_ENCODER_PATH)\n",
        "    scaler = joblib.load(SCALER_PATH)\n",
        "    \n",
        "    # Scale landmarks\n",
        "    test_landmarks_scaled = scaler.transform(test_landmarks)\n",
        "    \n",
        "    # Make predictions\n",
        "    print(\"Making predictions...\")\n",
        "    predictions = nn_model.predict(test_landmarks_scaled)\n",
        "    predicted_indices = np.argmax(predictions, axis=1)\n",
        "    predicted_labels_nn = [label_encoder.inverse_transform([i])[0] for i in predicted_indices]\n",
        "    \n",
        "    # Calculate accuracy\n",
        "    nn_accuracy = accuracy_score(test_labels, predicted_labels_nn)\n",
        "    print(f\"\\n✓ Landmark NN Test Accuracy: {nn_accuracy*100:.2f}%\")\n",
        "    \n",
        "    # Show predictions\n",
        "    print(\"\\nPredictions:\")\n",
        "    for true_label, pred_label, conf in zip(test_labels, predicted_labels_nn, np.max(predictions, axis=1)):\n",
        "        status = \"✓\" if true_label == pred_label else \"✗\"\n",
        "        print(f\"  {status} True: {true_label:8s} | Predicted: {pred_label:8s} | Confidence: {conf*100:.2f}%\")\n",
        "else:\n",
        "    print(f\"Landmark model not found. Please train using notebook 06.\")\n",
        "    nn_accuracy = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare both approaches\n",
        "if resnet_accuracy is not None and nn_accuracy is not None:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"MODEL COMPARISON SUMMARY\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"\\nApproach 1 - ResNet50 Transfer Learning:\")\n",
        "    print(f\"  Test Accuracy: {resnet_accuracy*100:.2f}%\")\n",
        "    print(f\"  Approach: Fine-tuned ResNet50 on raw images\")\n",
        "    print(f\"  Input: 96x96 RGB images\")\n",
        "    print(f\"  Model Size: ~23M parameters (base) + custom layers\")\n",
        "    \n",
        "    print(f\"\\nApproach 2 - Landmark Neural Network:\")\n",
        "    print(f\"  Test Accuracy: {nn_accuracy*100:.2f}%\")\n",
        "    print(f\"  Approach: Neural network on MediaPipe landmarks\")\n",
        "    print(f\"  Input: 63 landmark features (21 landmarks × 3 coords)\")\n",
        "    print(f\"  Model Size: Lightweight (~10K parameters)\")\n",
        "    \n",
        "    print(f\"\\n\" + \"-\"*60)\n",
        "    if resnet_accuracy > nn_accuracy:\n",
        "        diff = (resnet_accuracy - nn_accuracy) * 100\n",
        "        print(f\"✓ Winner: ResNet50 (+{diff:.2f}% higher accuracy)\")\n",
        "    elif nn_accuracy > resnet_accuracy:\n",
        "        diff = (nn_accuracy - resnet_accuracy) * 100\n",
        "        print(f\"✓ Winner: Landmark NN (+{diff:.2f}% higher accuracy)\")\n",
        "    else:\n",
        "        print(f\"Tie: Both models have equal accuracy\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Visualization\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
        "    models = ['ResNet50\\n(Transfer Learning)', 'Landmark NN\\n(MediaPipe)']\n",
        "    accuracies = [resnet_accuracy * 100, nn_accuracy * 100]\n",
        "    colors = ['#FF6B6B', '#4ECDC4']\n",
        "    \n",
        "    bars = ax.bar(models, accuracies, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
        "    ax.set_ylabel('Accuracy (%)', fontsize=12, fontweight='bold')\n",
        "    ax.set_title('Model Comparison - Test Set Performance', fontsize=14, fontweight='bold')\n",
        "    ax.set_ylim([0, 105])\n",
        "    ax.axhline(y=100, color='gray', linestyle='--', alpha=0.3)\n",
        "    ax.grid(axis='y', alpha=0.3)\n",
        "    \n",
        "    # Add value labels on bars\n",
        "    for bar, acc in zip(bars, accuracies):\n",
        "        height = bar.get_height()\n",
        "        ax.text(bar.get_x() + bar.get_width()/2., height + 2,\n",
        "                f'{acc:.1f}%', ha='center', va='bottom', fontweight='bold', fontsize=12)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Cannot compare - one or both models not trained yet\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Insights & Analysis\n",
        "\n",
        "### Approach Comparison\n",
        "\n",
        "**Approach 1: ResNet50 Transfer Learning**\n",
        "- ✓ **Strengths**: \n",
        "  - Learns directly from raw pixels\n",
        "  - Can capture subtle visual patterns\n",
        "  - Robust to different hand positions and sizes\n",
        "  - Proven architecture for image classification\n",
        "- ✗ **Weaknesses**: \n",
        "  - Requires significant computational resources\n",
        "  - Larger model size (~100MB+)\n",
        "  - Slower inference time\n",
        "  - Needs more training data and time\n",
        "\n",
        "**Approach 2: MediaPipe Landmarks + Neural Network**\n",
        "- ✓ **Strengths**: \n",
        "  - Extremely lightweight and fast\n",
        "  - Works on low-powered devices\n",
        "  - Small model size (~1MB)\n",
        "  - Interpretable features (hand landmarks)\n",
        "  - Translation-invariant (normalized to wrist)\n",
        "- ✗ **Weaknesses**: \n",
        "  - Dependent on MediaPipe's landmark detection accuracy\n",
        "  - May fail if hand detection fails\n",
        "  - Less robust to unusual hand poses\n",
        "\n",
        "### Recommendations for Real-World Deployment\n",
        "\n",
        "1. **For Mobile/Edge Devices**: Use Approach 2 (Landmark NN)\n",
        "   - Much faster inference\n",
        "   - Smaller memory footprint\n",
        "   - Still achieves good accuracy\n",
        "\n",
        "2. **For Server-Side/High Accuracy Needs**: Use Approach 1 (ResNet50)\n",
        "   - Higher potential accuracy\n",
        "   - More robust to edge cases\n",
        "   - Better for production systems with GPU access\n",
        "\n",
        "3. **Hybrid Approach** (Best of both worlds):\n",
        "   - Use MediaPipe for quick detection\n",
        "   - Fall back to ResNet50 when confidence is low\n",
        "   - Provides both speed and accuracy\n",
        "\n",
        "### Future Improvements\n",
        "\n",
        "- Expand from alphabet to words/phrases\n",
        "- Add temporal models (LSTM/Transformer) for continuous signing\n",
        "- Implement real-time webcam inference\n",
        "- Account for different lighting conditions\n",
        "- Handle multiple users with diverse hand sizes/skin tones\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
