{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24ba03c8-3b0f-492f-9331-8f1dbc8df4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44a17e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '../data/asl_alphabet_train/asl_alphabet_train'\n",
    "OUTPUT_FILE = '../data/asl_landmarks_train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0517a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MediaPipe will be initialized in parallel workers...\n"
     ]
    }
   ],
   "source": [
    "# Cell 3 is intentionally empty for parallel processing\n",
    "# MediaPipe is initialized inside each worker process (in landmark_processor.py)\n",
    "print(\"MediaPipe will be initialized in parallel workers...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccac8577-3025-41e2-b9ff-769cd5e0b734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 87000 images belonging to 29 classes.\n"
     ]
    }
   ],
   "source": [
    "image_paths = []\n",
    "labels = []\n",
    "\n",
    "for label in sorted(os.listdir(DATA_DIR)):\n",
    "    class_dir = os.path.join(DATA_DIR, label)\n",
    "    \n",
    "    if os.path.isdir(class_dir):\n",
    "        for image_file in os.listdir(class_dir):\n",
    "            if image_file.endswith(('.jpg', '.jpeg', '.png')):\n",
    "                image_paths.append(os.path.join(class_dir, image_file))\n",
    "                labels.append(label)\n",
    "\n",
    "print(f\"Found {len(image_paths)} images belonging to {len(set(labels))} classes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "396e9aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting PARALLEL processing with thread-local models...\n",
      "Using 8 threads (each with its own MediaPipe model)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1764036467.117343 1430463 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 90.5), renderer: Apple M3\n",
      "I0000 00:00:1764036467.122400 1430473 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 90.5), renderer: Apple M3\n",
      "I0000 00:00:1764036467.123312 1430454 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 90.5), renderer: Apple M3\n",
      "I0000 00:00:1764036467.127368 1430482 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 90.5), renderer: Apple M3\n",
      "I0000 00:00:1764036467.130801 1430491 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 90.5), renderer: Apple M3\n",
      "I0000 00:00:1764036467.138598 1430500 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 90.5), renderer: Apple M3\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "I0000 00:00:1764036467.156416 1430509 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 90.5), renderer: Apple M3\n",
      "I0000 00:00:1764036467.159349 1430518 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 90.5), renderer: Apple M3\n",
      "W0000 00:00:1764036467.200741 1430456 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1764036467.202228 1430486 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1764036467.209374 1430467 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1764036467.212648 1430492 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1764036467.214177 1430501 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1764036467.214605 1430476 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1764036467.222754 1430512 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1764036467.223941 1430520 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1764036467.248765 1430476 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1764036467.249501 1430486 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1764036467.257171 1430462 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1764036467.267114 1430487 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n",
      "W0000 00:00:1764036467.267523 1430467 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1764036467.267837 1430492 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1764036467.273391 1430513 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1764036467.273576 1430503 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1764036467.274378 1430524 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a5c3c09d8bd40fc9d6ca1714d0b7f72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting landmarks:   0%|          | 0/87000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 87000 images.\n",
      "   Successful: 63676 (73.19%)\n",
      "   Failed: 23324 (26.81%)\n"
     ]
    }
   ],
   "source": [
    "# PARALLEL PROCESSING with ThreadPoolExecutor + Thread-Local MediaPipe\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import threading\n",
    "import os\n",
    "import numpy as np\n",
    "import mediapipe as mp_module\n",
    "import cv2\n",
    "\n",
    "# Thread-local storage - each thread gets its own MediaPipe model\n",
    "thread_local = threading.local()\n",
    "\n",
    "def get_hands_model():\n",
    "    \"\"\"Get or create a MediaPipe Hands model for the current thread\"\"\"\n",
    "    if not hasattr(thread_local, 'hands_model'):\n",
    "        mp_hands = mp_module.solutions.hands\n",
    "        thread_local.hands_model = mp_hands.Hands(\n",
    "            static_image_mode=True,\n",
    "            max_num_hands=1,\n",
    "            min_detection_confidence=0.5\n",
    "        )\n",
    "    return thread_local.hands_model\n",
    "\n",
    "def process_single_image(file_path):\n",
    "    \"\"\"Process one image using thread-local MediaPipe model\"\"\"\n",
    "    try:\n",
    "        # Get this thread's own MediaPipe model\n",
    "        hands_model = get_hands_model()\n",
    "        \n",
    "        image = cv2.imread(file_path)\n",
    "        if image is None:\n",
    "            return [np.nan] * 63\n",
    "\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        results = hands_model.process(image_rgb)\n",
    "\n",
    "        if results.multi_hand_landmarks:\n",
    "            hand_landmarks = results.multi_hand_landmarks[0]\n",
    "            wrist_coords = hand_landmarks.landmark[0]\n",
    "\n",
    "            landmark_row = []\n",
    "            for landmark in hand_landmarks.landmark:\n",
    "                relative_x = landmark.x - wrist_coords.x\n",
    "                relative_y = landmark.y - wrist_coords.y\n",
    "                relative_z = landmark.z - wrist_coords.z\n",
    "                landmark_row.extend([relative_x, relative_y, relative_z])\n",
    "            return landmark_row\n",
    "        else:\n",
    "            return [np.nan] * 63\n",
    "    except Exception as e:\n",
    "        return [np.nan] * 63\n",
    "\n",
    "# Run with ThreadPoolExecutor\n",
    "print(\"Starting PARALLEL processing with thread-local models...\")\n",
    "num_workers = os.cpu_count()\n",
    "print(f\"Using {num_workers} threads (each with its own MediaPipe model)\")\n",
    "\n",
    "processed_data = []\n",
    "with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "    # Submit all tasks\n",
    "    futures = {executor.submit(process_single_image, path): i for i, path in enumerate(image_paths)}\n",
    "    \n",
    "    # Collect results with progress bar\n",
    "    for future in tqdm(as_completed(futures), total=len(image_paths), desc=\"Extracting landmarks\"):\n",
    "        idx = futures[future]\n",
    "        processed_data.append((idx, future.result()))\n",
    "\n",
    "# Sort by original index (as_completed returns in completion order)\n",
    "processed_data.sort(key=lambda x: x[0])\n",
    "processed_data = [x[1] for x in processed_data]\n",
    "\n",
    "# Statistics\n",
    "successful_count = sum(1 for row in processed_data if not np.isnan(row[0]))\n",
    "failed_count = len(processed_data) - successful_count\n",
    "\n",
    "print(f\"\\nProcessed {len(processed_data)} images.\")\n",
    "print(f\"   Successful: {successful_count} ({successful_count/len(processed_data)*100:.2f}%)\")\n",
    "print(f\"   Failed: {failed_count} ({failed_count/len(processed_data)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be60ed1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
