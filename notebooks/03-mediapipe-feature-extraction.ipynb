{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24ba03c8-3b0f-492f-9331-8f1dbc8df4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44a17e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '../data/asl_alphabet_train/asl_alphabet_train'\n",
    "OUTPUT_FILE = '../data/asl_landmarks_train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d0517a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MediaPipe will be initialized in parallel workers...\n"
     ]
    }
   ],
   "source": [
    "# Cell 3 is intentionally empty for parallel processing\n",
    "# MediaPipe is initialized inside each worker process (in landmark_processor.py)\n",
    "print(\"MediaPipe will be initialized in parallel workers...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ccac8577-3025-41e2-b9ff-769cd5e0b734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 87000 images belonging to 29 classes.\n"
     ]
    }
   ],
   "source": [
    "image_paths = []\n",
    "labels = []\n",
    "\n",
    "for label in sorted(os.listdir(DATA_DIR)):\n",
    "    class_dir = os.path.join(DATA_DIR, label)\n",
    "    \n",
    "    if os.path.isdir(class_dir):\n",
    "        for image_file in os.listdir(class_dir):\n",
    "            if image_file.endswith(('.jpg', '.jpeg', '.png')):\n",
    "                image_paths.append(os.path.join(class_dir, image_file))\n",
    "                labels.append(label)\n",
    "\n",
    "print(f\"Found {len(image_paths)} images belonging to {len(set(labels))} classes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "396e9aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting PARALLEL processing with thread-local models...\n",
      "Using 12 threads (each with its own MediaPipe model)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1763698419.926768 51879873 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M4 Pro\n",
      "I0000 00:00:1763698419.931491 51879887 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M4 Pro\n",
      "W0000 00:00:1763698419.931668 51879874 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1763698419.935213 51879900 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M4 Pro\n",
      "W0000 00:00:1763698419.936242 51879884 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1763698419.936797 51879891 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1763698419.940187 51879913 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M4 Pro\n",
      "W0000 00:00:1763698419.941188 51879901 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1763698419.941885 51879891 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1763698419.944970 51879926 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M4 Pro\n",
      "W0000 00:00:1763698419.946172 51879901 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1763698419.947026 51879914 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1763698419.950641 51879939 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M4 Pro\n",
      "W0000 00:00:1763698419.950732 51879928 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1763698419.952022 51879914 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1763698419.955201 51879928 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1763698419.955942 51879940 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1763698419.957015 51879952 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M4 Pro\n",
      "W0000 00:00:1763698419.961150 51879940 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1763698419.961229 51879966 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M4 Pro\n",
      "W0000 00:00:1763698419.962705 51879956 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1763698419.965945 51879979 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M4 Pro\n",
      "W0000 00:00:1763698419.967602 51879969 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1763698419.968655 51879956 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1763698419.970189 51879992 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M4 Pro\n",
      "W0000 00:00:1763698419.972553 51879969 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1763698419.973581 51879981 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1763698419.974936 51880005 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M4 Pro\n",
      "W0000 00:00:1763698419.976422 51879996 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1763698419.978960 51879981 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1763698419.980504 51880007 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1763698419.981292 51880018 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M4 Pro\n",
      "W0000 00:00:1763698419.981364 51879996 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1763698419.987070 51880013 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1763698419.987576 51880019 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1763698419.992040 51880019 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b20b46918464e179132b824bf91f2f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting landmarks:   0%|          | 0/87000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 87000 images.\n",
      "   Successful: 63676 (73.19%)\n",
      "   Failed: 23324 (26.81%)\n"
     ]
    }
   ],
   "source": [
    "# PARALLEL PROCESSING with ThreadPoolExecutor + Thread-Local MediaPipe\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import threading\n",
    "import os\n",
    "import numpy as np\n",
    "import mediapipe as mp_module\n",
    "import cv2\n",
    "\n",
    "# Thread-local storage - each thread gets its own MediaPipe model\n",
    "thread_local = threading.local()\n",
    "\n",
    "def get_hands_model():\n",
    "    \"\"\"Get or create a MediaPipe Hands model for the current thread\"\"\"\n",
    "    if not hasattr(thread_local, 'hands_model'):\n",
    "        mp_hands = mp_module.solutions.hands\n",
    "        thread_local.hands_model = mp_hands.Hands(\n",
    "            static_image_mode=True,\n",
    "            max_num_hands=1,\n",
    "            min_detection_confidence=0.5\n",
    "        )\n",
    "    return thread_local.hands_model\n",
    "\n",
    "def process_single_image(file_path):\n",
    "    \"\"\"Process one image using thread-local MediaPipe model\"\"\"\n",
    "    try:\n",
    "        # Get this thread's own MediaPipe model\n",
    "        hands_model = get_hands_model()\n",
    "        \n",
    "        image = cv2.imread(file_path)\n",
    "        if image is None:\n",
    "            return [np.nan] * 63\n",
    "\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        results = hands_model.process(image_rgb)\n",
    "\n",
    "        if results.multi_hand_landmarks:\n",
    "            hand_landmarks = results.multi_hand_landmarks[0]\n",
    "            wrist_coords = hand_landmarks.landmark[0]\n",
    "\n",
    "            landmark_row = []\n",
    "            for landmark in hand_landmarks.landmark:\n",
    "                relative_x = landmark.x - wrist_coords.x\n",
    "                relative_y = landmark.y - wrist_coords.y\n",
    "                relative_z = landmark.z - wrist_coords.z\n",
    "                landmark_row.extend([relative_x, relative_y, relative_z])\n",
    "            return landmark_row\n",
    "        else:\n",
    "            return [np.nan] * 63\n",
    "    except Exception as e:\n",
    "        return [np.nan] * 63\n",
    "\n",
    "# Run with ThreadPoolExecutor\n",
    "print(\"Starting PARALLEL processing with thread-local models...\")\n",
    "num_workers = os.cpu_count()\n",
    "print(f\"Using {num_workers} threads (each with its own MediaPipe model)\")\n",
    "\n",
    "processed_data = []\n",
    "with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "    # Submit all tasks\n",
    "    futures = {executor.submit(process_single_image, path): i for i, path in enumerate(image_paths)}\n",
    "    \n",
    "    # Collect results with progress bar\n",
    "    for future in tqdm(as_completed(futures), total=len(image_paths), desc=\"Extracting landmarks\"):\n",
    "        idx = futures[future]\n",
    "        processed_data.append((idx, future.result()))\n",
    "\n",
    "# Sort by original index (as_completed returns in completion order)\n",
    "processed_data.sort(key=lambda x: x[0])\n",
    "processed_data = [x[1] for x in processed_data]\n",
    "\n",
    "# Statistics\n",
    "successful_count = sum(1 for row in processed_data if not np.isnan(row[0]))\n",
    "failed_count = len(processed_data) - successful_count\n",
    "\n",
    "print(f\"\\nProcessed {len(processed_data)} images.\")\n",
    "print(f\"   Successful: {successful_count} ({successful_count/len(processed_data)*100:.2f}%)\")\n",
    "print(f\"   Failed: {failed_count} ({failed_count/len(processed_data)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be60ed1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asl_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
